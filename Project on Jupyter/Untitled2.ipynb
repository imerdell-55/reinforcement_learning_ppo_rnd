{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ajOWps4egYep","colab_type":"code","outputId":"c30e5d8f-cf8e-40d8-e81e-86049a323f6b","executionInfo":{"status":"error","timestamp":1566514260757,"user_tz":-420,"elapsed":9301701,"user":{"displayName":"Wisnu Nugroho","photoUrl":"","userId":"18085654963667627844"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1zSw11qlyCTcaW8cLfqDiyvSTOR72Tmz4"}},"source":["import gym\n","from gym.envs.registration import register\n","    \n","import torch\n","import torch.nn as nn\n","from torch.distributions import Categorical\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n","dataType = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n","      \n","class PPO_Model(nn.Module):\n","    def __init__(self, state_dim, action_dim):\n","        super(PPO_Model, self).__init__()\n","        \n","        # Actor\n","        self.actor_layer = nn.Sequential(\n","                nn.Linear(state_dim, 640),\n","                nn.ELU(),\n","                nn.Linear(640, 640),\n","                nn.ELU(),\n","                nn.Linear(640, 64),\n","                nn.ELU(),\n","                nn.Linear(64, action_dim),\n","                nn.Softmax(-1)\n","              ).float().to(device)\n","        \n","        # Intrinsic Critic\n","        self.value_layer = nn.Sequential(\n","                nn.Linear(state_dim, 640),\n","                nn.ELU(),\n","                nn.Linear(640, 640),\n","                nn.ELU(),\n","                nn.Linear(640, 64),\n","                nn.ELU(),\n","                nn.Linear(64, 1)\n","              ).float().to(device)\n","        \n","    # Init wieghts to make training faster\n","    # But don't init weight if you load weight from file\n","    def lets_init_weights(self):      \n","        self.actor_layer.apply(self.init_weights)\n","        self.value_layer.apply(self.init_weights)\n","        \n","    def init_weights(self, m):\n","        for name, param in m.named_parameters():\n","            if 'bias' in name:\n","               nn.init.constant_(param, 0.01)\n","            elif 'weight' in name:\n","                nn.init.kaiming_uniform_(param, mode = 'fan_in', nonlinearity = 'relu')\n","        \n","    def forward(self, state, is_act = False):\n","        if is_act: \n","            return self.actor_layer(state)\n","        else:\n","            return self.actor_layer(state), self.value_layer(state)\n","\n","class Memory:\n","    def __init__(self):\n","        self.actions = []\n","        self.states = []\n","        self.logprobs = []\n","        self.rewards = []\n","        self.dones = []     \n","        self.next_states = []\n","        \n","    def save_eps(self, state, reward, next_states, done):\n","        self.rewards.append(reward)\n","        self.states.append(state)\n","        self.dones.append(done)\n","        self.next_states.append(next_states)\n","        \n","    def save_actions(self, action):\n","        self.actions.append(action)\n","        \n","    def save_logprobs(self, logprob):\n","        self.logprobs.append(logprob)\n","        \n","    def clearMemory(self):\n","        del self.actions[:]\n","        del self.states[:]\n","        del self.logprobs[:]\n","        del self.rewards[:]\n","        del self.dones[:]\n","        del self.next_states[:]\n","                \n","class Utils:\n","    def __init__(self):\n","        self.gamma = 0.95\n","        self.lam = 0.99\n","\n","    # Categorical Distribution is used for Discrete Action Environment\n","    # The neural network output the probability of actions (Stochastic policy), then pass it to Categorical Distribution\n","    \n","    def sample(self, datas):\n","        distribution = Categorical(datas)      \n","        return distribution.sample().float().to(device)\n","        \n","    def entropy(self, datas):\n","        distribution = Categorical(datas)            \n","        return distribution.entropy().float().to(device)\n","      \n","    def logprob(self, datas, value_data):\n","        distribution = Categorical(datas)\n","        return distribution.log_prob(value_data).float().to(device)      \n","      \n","    def normalize(self, data):\n","        data_normalized = (data - torch.mean(data)) / torch.std(data)\n","        return data_normalized\n","      \n","    def to_numpy(self, datas):\n","        if torch.cuda.is_available():\n","            datas = datas.cpu().detach().numpy()\n","        else:\n","            datas = datas.detach().numpy()            \n","        return datas        \n","      \n","    def discounted(self, datas):\n","        # Discounting future reward        \n","        discounted_datas = torch.zeros_like(datas)\n","        running_add = 0\n","        \n","        for i in reversed(range(len(datas))):\n","            running_add = running_add * self.gamma + datas[i]\n","            discounted_datas[i] = running_add\n","            \n","        return discounted_datas\n","      \n","    def q_values(self, reward, next_value, done, value_function):\n","        # Finding Q Values\n","        # Q = R + V(St+1)\n","        q_values = reward + (1 - done) * self.gamma * next_value           \n","        return q_values\n","      \n","    def compute_GAE(self, values, rewards, next_value, done):\n","        # Computing general advantages estimator\n","        gae = 0\n","        returns = []\n","        \n","        for step in reversed(range(len(rewards))):   \n","            delta = rewards[step] + self.gamma * next_value[step] * (1 - done[step]) - values[step]\n","            gae = delta + self.lam * gae\n","            returns.append(gae.detach())\n","            \n","        return torch.stack(returns)\n","\n","    def prepro(self, I):\n","        I = I[35:195] # crop\n","        I = I[::2,::2, 0] # downsample by factor of 2\n","        I[I == 144] = 0 # erase background (background type 1)\n","        I[I == 109] = 0 # erase background (background type 2)\n","        I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n","        \n","        X = I.astype(np.float32).ravel() # Combine items in 1 array \n","        return X\n","        \n","class Agent:  \n","    def __init__(self, state_dim, action_dim):        \n","        self.policy_clip = 0.2 \n","        self.value_clip = 0.2      \n","        self.entropy_coef = 0.01\n","        self.vf_loss_coef = 1\n","\n","        self.PPO_epochs = 3\n","        \n","        self.policy = PPO_Model(state_dim, action_dim)\n","        self.policy_old = PPO_Model(state_dim, action_dim)\n","        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr = 0.0001)\n","\n","        self.memory = Memory()\n","        self.utils = Utils()        \n","        \n","    def save_eps(self, state, reward, next_states, done):\n","        self.memory.save_eps(state, reward, next_states, done)\n","\n","    # Loss for PPO\n","    def get_loss(self, old_states, old_actions, rewards, old_next_states, dones):      \n","        action_probs, value  = self.policy(old_states)  \n","        old_action_probs, old_value = self.policy_old(old_states)\n","        _, next_value = self.policy(old_next_states)\n","        \n","        # Don't update old value\n","        old_action_probs = old_action_probs.detach()\n","        old_value = old_value.detach()\n","\n","        # Don't update next value\n","        next_value = next_value.detach()\n","                \n","        # Getting entropy from the action probability\n","        dist_entropy = self.utils.entropy(action_probs).mean()\n","\n","        # Getting external general advantages estimator\n","        rewards = rewards.detach()\n","        advantages = self.utils.compute_GAE(value, rewards, next_value, dones).detach()\n","        \n","        # Getting External critic loss by using Clipped critic value\n","        vpredclipped = old_value + torch.clamp(value - old_value, -self.value_clip, self.value_clip) # Minimize the difference between old value and new value\n","        vf_losses1 = (rewards - value).pow(2) # Mean Squared Error\n","        vf_losses2 = (rewards - vpredclipped).pow(2) # Mean Squared Error\n","        critic_loss = torch.min(vf_losses1, vf_losses2).mean()\n","\n","        # Finding the ratio (pi_theta / pi_theta__old):  \n","        logprobs = self.utils.logprob(action_probs, old_actions) \n","        old_logprobs = self.utils.logprob(old_action_probs, old_actions).detach()\n","        \n","        # Finding Surrogate Loss:\n","        ratios = torch.exp(logprobs - old_logprobs) # ratios = old_logprobs / logprobs\n","        surr1 = ratios * advantages\n","        surr2 = torch.clamp(ratios, 1 - self.policy_clip, 1 + self.policy_clip) * advantages\n","        pg_loss = torch.min(surr1, surr2).mean()           \n","        \n","        # We need to maximaze Policy Loss to make agent always find Better Rewards\n","        # and minimize Critic Loss and \n","        loss = (critic_loss * self.vf_loss_coef) - (dist_entropy * self.entropy_coef) - pg_loss \n","                \n","        return loss       \n","      \n","    def act(self, state):\n","        state = torch.FloatTensor(state).to(device)      \n","        action_probs = self.policy_old(state, is_act = True)\n","        \n","        # Sample the action\n","        action = self.utils.sample(action_probs)\n","        \n","        self.memory.save_actions(action)         \n","        return action.item()\n","        \n","    # Update the PPO part (the actor and value)\n","    def update_ppo(self):        \n","        length = len(self.memory.states)\n","\n","        # Convert list in tensor\n","        old_states = torch.FloatTensor(self.memory.states).to(device).detach()\n","        old_actions = torch.FloatTensor(self.memory.actions).to(device).detach()\n","        old_next_states = torch.FloatTensor(self.memory.next_states).to(device).detach()\n","        dones = torch.FloatTensor(self.memory.dones).view(length, 1).view(length, 1).to(device).detach() \n","        rewards = torch.FloatTensor(self.memory.rewards).view(length, 1).view(length, 1).to(device).detach()\n","                \n","        # Optimize policy for K epochs:\n","        for epoch in range(self.PPO_epochs):\n","            loss = self.get_loss(old_states, old_actions, rewards, old_next_states, dones)          \n","                        \n","            self.policy_optimizer.zero_grad()\n","            loss.backward()                    \n","            self.policy_optimizer.step() \n","\n","        # Clear state, action, reward in memory    \n","        self.memory.clearMemory()\n","        \n","        # Copy new weights into old policy:\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","        \n","    def save_weights(self):\n","        torch.save(self.policy.state_dict(), '/test/My Drive/Rl_PPO_Pong/actor_pong_ppo_rnd.pth')\n","        torch.save(self.policy_old.state_dict(), '/test/My Drive/Rl_PPO_Pong/old_actor_pong_ppo_rnd.pth')\n","        \n","    def load_weights(self):\n","        self.policy.load_state_dict(torch.load('/test/My Drive/Rl_PPO_Pong/actor_pong_ppo_rnd.pth', map_location='cpu'))        \n","        self.policy_old.load_state_dict(torch.load('/test/My Drive/Rl_PPO_Pong/old_actor_pong_ppo_rnd.pth', map_location='cpu'))   \n","        \n","    def lets_init_weights(self):\n","        self.policy.lets_init_weights()\n","        self.policy_old.lets_init_weights()\n","        \n","def plot(datas):\n","    print('----------')\n","    \n","    plt.plot(datas)\n","    plt.plot()\n","    plt.xlabel('Episode')\n","    plt.ylabel('Datas')\n","    plt.show()\n","    \n","    print('Max :', np.max(datas))\n","    print('Min :', np.min(datas))\n","    print('Avg :', np.mean(datas))\n","\n","def run_episode(env, agent, state_dim, render, training_mode):\n","    utils = Utils()\n","    ############################################\n","    state = env.reset()\n","    state = utils.prepro(state)\n","\n","    done = False\n","    total_reward = 0\n","    t = 0\n","    ############################################\n","    \n","    while not done:\n","        # Running policy_old:            \n","        action = int(agent.act(state))\n","        state_n, reward, done, info = env.step(action)\n","        state_n = utils.prepro(state_n)\n","        state_n = state_n - state\n","        \n","        t += 1                       \n","        total_reward += reward    \n","          \n","        if training_mode:\n","            agent.save_eps(state, reward, state_n, done) \n","            \n","        state = state_n     \n","                \n","        if render:\n","            env.render()\n","        if done:\n","            return total_reward, t\n","    \n","def main():\n","    ############## Hyperparameters ##############\n","    using_google_drive = True # If you using Google Colab and want to save the agent to your GDrive, set this to True\n","    load_weights = True # If you want to load the agent, set this to True\n","    save_weights = True # If you want to save the agent, set this to True\n","    training_mode = True # If you want to train the agent, set this to True. But set this otherwise if you only want to test it\n","    reward_threshold = None # Set threshold for reward. The learning will stop if reward has pass threshold. Set none to sei this off\n","    \n","    render = False # If you want to display the image. Turn this off if you run this in Google Collab\n","    n_update = 1 # How many episode before you update the Policy\n","    n_plot_batch = 100 # How many episode you want to plot the result\n","    n_episode = 10000 # How many episode you want to run\n","    #############################################         \n","    env_name = \"Pong-v0\"\n","    env = gym.make(env_name)\n","    state_dim = 6400\n","    action_dim = env.action_space.n\n","        \n","    utils = Utils()     \n","    agent = Agent(state_dim, action_dim)  \n","    ############################################# \n","    \n","    if using_google_drive:\n","        from google.colab import drive\n","        drive.mount('/test')\n","    \n","    if load_weights:\n","        agent.load_weights()\n","        print('Weight Loaded')\n","    else :\n","        agent.lets_init_weights()\n","        print('Init Weight')\n","    \n","    if torch.cuda.is_available() :\n","        print('Using GPU')\n","    \n","    rewards = []   \n","    batch_rewards = []\n","    batch_solved_reward = []\n","    \n","    times = []\n","    batch_times = []\n","    \n","    for i_episode in range(1, n_episode):\n","        total_reward, time = run_episode(env, agent, state_dim, render, training_mode)\n","        print('Episode {} \\t t_reward: {} \\t time: {} \\t '.format(i_episode, total_reward, time))\n","        batch_rewards.append(total_reward)\n","        batch_times.append(time)       \n","        \n","        if training_mode:\n","            # update after n episodes\n","            if i_episode % n_update == 0 and i_episode != 0:\n","                agent.update_ppo()\n","                print('Agent has been updated')\n","\n","                if save_weights:\n","                    agent.save_weights()\n","                    print('Weights saved')\n","                    \n","        if reward_threshold:\n","            if len(batch_solved_reward) == 100:            \n","                if np.mean(batch_solved_reward) >= reward_threshold :              \n","                    for reward in batch_times:\n","                        rewards.append(reward)\n","\n","                    for time in batch_rewards:\n","                        times.append(time)                    \n","\n","                    print('You solved task after {} episode'.format(len(rewards)))\n","                    break\n","\n","                else:\n","                    del batch_solved_reward[0]\n","                    batch_solved_reward.append(total_reward)\n","\n","            else:\n","                batch_solved_reward.append(total_reward)\n","            \n","        if i_episode % n_plot_batch == 0 and i_episode != 0:\n","            # Plot the reward, times for every n_plot_batch\n","            plot(batch_rewards)\n","            plot(batch_times)\n","            \n","            for reward in batch_times:\n","                rewards.append(reward)\n","                \n","            for time in batch_rewards:\n","                times.append(time)\n","                \n","            batch_rewards = []\n","            batch_times = []\n","\n","            print('========== Cummulative ==========')\n","            # Plot the reward, times for every episode\n","            plot(rewards)\n","            plot(times)\n","            \n","    print('========== Final ==========')\n","     # Plot the reward, times for every episode\n","    plot(rewards)\n","    plot(times) \n","            \n","if __name__ == '__main__':\n","    main()"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}